T-SNE
	from n-dim space to m-dim space (m<<n)
SVD
	Сингулярное разложение
	Извлечение "важное" информации
	это по сути убирание корреляции из исходных данных
Face detection
	каскадные классификаторы (отбор из кучи признаков adaboost'ом), включены в OpenCV
		также работают для глаз, ушей и так далее
	разные алгоритмы классификации: распеределение пикселей (два варианта), локальное сравнение гистрограм цветов
	ROC-кривая, F-мера, соревнование с кучей эллипсов
McNemar test
	проверяет качество (я слышал про распознование цифр) лучше числа ошибок
	сравнение моделей
	строится 2x2 таблица (модель 1 да-нет, модель 2 да-нет) по побочной диагонали смотрим и замеряем, 
	если одна покрывает почти все ошибки другой, то она значительно лучше
набор данных
	генерация синтектики вкупе с обычными заметно помогает и спасает от переобучения
		актуально для огромных сетей и данных
	ImageNet
	рассматриваем куски изображения (большие)
функции активации нейрона
	сигмойдная - 1/(1+e^-t)
	max(0,x) - !!!!!!!!! хорошая на практике и быстро обучается (coursera + klaus)
	гиперболический тангенс
dropout в нейросетях
	мы не рассматриваем случайную часть нейронов во внутренних слоях для каждого входного объекта
	так мы снижаем переобучение -> мы не модифицируем все подряд веса
стохастический Градиентный спуск
	мини-пакеты
		репрезентативно отображают выборку (пропорционально кол-ву/размерам классов)
		можно параллелить
		помогает при избыточных данных
		сгенеририровать эти данные заранее, чтобы при обучении не тратить время на это
	темп обучения
		если колебания и нет стабильного роста - уменьшаем
		если медленно улучшается - увеличиваем
		ближе к концу есть смысл уменьшать
		можно отслеживать результаты на validation set'е, но потом проверять на отдельном testset'е
word2vec
	если слово слишком редкое или слишком частое, то у него будет плохой вектор (непоказательный)
		точно правда для редкости)))
	he/she blizko tak kak mozhno ih zamenit' v predlozhenii
Обучение сети
	начальные веса случайные, небольшие, обратно пропорционально числу входящих(1/sqrt(income or fan-in))
	сдвигать веса к нулям (средний вес должен быть 0)
	scale весов
	big learning rate - есть вероятность встрять на плато (производная около 0)
	пример плато в классификации - это когда output - число единиц/число объектов (cross-entropy, square error)
	не спеши снижать темп обучения
	ускорение
		юзай инерцию (градиент меняет скорость)
			полезно для скорости обучения (выруливает на нужное направление)
			просто записывается (мы храним дельты прошлые и все!)
			в начале большая инерция может не туда вести
			итого: маленьая в начале и постепенно приближай к 1 (momentum < 1 !!!)
			метод 1: 
				первый шаг в направлении суммы градиентов (большой)
				потом шаг просто по градиенту (маленький)
				другими словами мы меряем градиент не текущей точке а в destination of jump
			метод 2:
				считать градиент в точке и туда же идти
			метод 1 лучше: сначала рискуем потом корректируем, 
			хуже если корректируем а потом рискуем (метод 2)
		медленно подбирай темп обучения, темп обучения свой для каждого???
			если градиент скачит, то увеличивай
				если градиент не меняется, то уменьшай
			мы можем сильно поменять веса, когда input маленький
			для каждого веса свой темп обучения Gij (1 as start, мы его на глобальный вес множим)
				если градиент два раза не меняет знак то увеличиваем вес (+немного)
					если меняет знак то уменьшаем (*coeff < 1)
				ограничивай этот вес
		rprop темп обучения делить на среднее колибание градиента этого параметра
			увеличиваем шаг, если градиент сохранил знак (мультипликативно)
				если знак изменился, то уменьшаем шаг (мультипликативно)
			работает только с full batch'ем
				или очень большими mini
				если у нас по каким-то batch'ам плюс мало, а в одном плюс много, то мы вес не туда уведем
		rmsprop
			храним средний градиент = лин комбинация прошлого (0.9 коэф) и квадрата текущего (0.1)
			делим градиент на корень среднего
			используем вместо локального темпа обучения
		методы оптимизации, которые учитывают кривизну??
	если мало данных то full-batch или если данные не избыточны
		adaptive learning rate, rprop
	если много данных то mini-batch
		используем инерцию
		rmsprop
		yan lecun поседние исследования/адаптировать каждый вес отдельно
Gaussian Process models
	????????????????
Байесовские Методы (!!!!):
	Bayesian Additive Regression Trees (BART), где заключительная модель определена с точки зрения суммы по многим слабым ученикам 
	(мало чем отличающийся от методов ансамбля), реализованы в пакете BayesTree. Байесовская нестационарная, полупараметрическая нелинейная 
	регрессия и проектирование с помощью древовидного Гауссовского процесса, включая Байесовский CART и древовидной линейные 
	модели, доступны в пакете tgp.
dropout
	мы случайно выкидываем половину весов из сети для каждого элемента
	во время test time мы делим все веса пополам
	можно выкидывать и input но с маленькой вероятностью
	помогает от overfitting
	early stop + dropout = ok
пробуй линейную модель или SVM с ядром как первую идею
	SVM не переобучается сильно
RBM
  Для распознавания, первые два слоя unsupervised, потом добавляем label.
  
