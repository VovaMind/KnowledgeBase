# Заметки с EACL 2017

##### Содержание
[Word embeddings](#word_embeddings)  
[Summarization](#summarization)  
[Multiwords expressions](#miltiword)  
[Universal dependencies](#ud)  
[Probability topic models and user behavior](#topicmodel)  
[Coreference](#coreference)  
[Information retrieval and information extraction](#ir)  

<a name="word_embeddings"/>

# Word embeddings (word vector space)

[Слайды](http://people.ds.cam.ac.uk/iv250/tutorial/wv-tutorial.pdf).

## Введение
Иногда word vector space сложно использовать в приложениях - антонимы находятся рядом. Например, мужчина-женщина, запад-восток и так далее.

http://babelnet.org/ - мулиязычный ресурс со словарями, семантикой и базой знаний, synset'ы.

http://paraphrase.org/ - ресурс с большим набором перефразировок.

Unsupervised подходы дешевле - не нужен размеченный корпус.

Нужно объединение двух подходов: unsupervised + размеченные ресурсы.

## Обучение по контексту
Хотим извлекать признаки, которые будут примеными ко многим задачам и инварианты к языку.

Соотвествие слово->вектор в многомерном пространстве.

Слова имеет близкие значения, если они встречаются в одинаковом контексте.

Mikolov статья 2013 года.

Можно использовать негативные примеры в обучении. (случайное слово в контесте из корпуса).

Типы контекстов: bag of words (BOW); с позициями относительно текущего слова; с синтаксическими зависимостями; с синтаксическими зависимостями, но связи с предлогами удаляются.

Cross language dependency based spaces (EACL 2017 статья).

Больше веса для глаголов и пралагательных в контексте, это помогает для улучшения качества. Статья Schwartz NAACL 20016.

Для разных задач помогают разные типы контекстов. Также можно играть с размерностью пространства.

Melamud CONLL 2016, использовал bidirectional LTSM.

Симметричные pattern'ы, статья Schwartz CONLL 2015.

Context2vec.

## Лингвистические ограничения
Все зависит от корпуса.

Используем структурированые знания (источники?).

Wordnet - это набор synset'ов, каждый из которых соответствует некоторому концепту. Слово может участвовать в нескольких synset'ах. Внутри wordnet'а есть отношения: гипероним, гипоним, принадлежность к domain'у, мероним (часть-целое) и так далее. Гиперонимы образуют иерархию. Прилагательные сложно ложаться на иерархию.

Проблемы источников типа wordnet: дорого обновлять, ограниченный словарь, плохо описывают конкретные предметные области и последние изменения в языке.

Wikipedia полезный источник данных. Есть ссылки, многоязычность много сырой и структурированной информации.

Wictionary намного больше, чем wordnet, хорошое покрытие языков, очень избыточные.

omegawiki близко к wordnet'у, плохое покрытие.

Bablenet - это слияние всех источников, похоже на wordnet, есть API. Есть live-версия (последняя, видимо, не очень стабильная).

Paraphrase база с шаблонами на 16 языках, извлекалась из параллельных корпусов. Несколько типов: лексический, перефразировка, синтаксический с нетерминалами. В базе есть scores.

Google ngram база. Ранжированные перефразировки.

Wordnet имеет хорошее качество.

## Комбинирование подходов
Обобщать по конкретному словарю сложно.

Можно соединить два подхода. Основные способы: "тяжелый" (все делаем совместно), пост-обработка (после построения векторов двигаем их).

Оба подхода используют одинаковые входные данные.

Yu and Dredze ACL 2014.

Интерполяции по источникам знаний.

Парное сходство по примерам. Fool/stupid ближе, чем fool/smart.

Нужно отличать similarity, relatedness и association. Для классификации и topic modeling можно не отличать.

Пост-обработка. Двигаем слова после построения векторов и оптимизируем фунцию для пар синонимов. При этом не должны сильно поменять построенные вектора.

Mrksic NAACL 2016. Функция оценки, которая базируется на синонимах, антонимах и исходных векторах.

Wordnet хорошо для одноязычного случая, babelnet для многоязычного.

Paragram подход (Wieting 2016) добавляем "негативные" пары синонимов (вроде бы, случайные).

Ограничения для конкретной модели помогают с омонимией.

## Последняя часть
Если использовать только ограничения и случайные вектора, то это может неплохо работать (очень зависят от языка).

Многоязычные вектора работают неплохо даже при условии недостатка ресурсов.

SimLex данные (есть для русского). Не для всех языков хватает данных.

Диалог + отслеживание его состояния (slots, тип вопроса). Рассматриваем диалог как распределение таких состояний. Делать по точным совпадаением помогает очень плохо. Семантичесие словари помогают для маленьких данных. Можно обучить нейросеть.

offtop: можно перейти от классификации на много классов к большому количеству бинарных классификаций. Relu фунция активации max(0,x). CNN помогает обучать представления. Сегменты слов в NN. 

Wizard of Oz dialog dataset.

Glove vectors.

Морфология с небольшой коллекцией размеченных данные работает хорошо. Оченб помогает для немецкого.

Vendrov ICLR 2016 (частичный порядок в векторном пространстве).

## Омонимия
Есть проблема с омонимией.

Разделять значения для слов. 

Можно делать unsupervised кластеризуя слова по контекстам. Нужно зарнее выбрать число кластеров (число значений) для каждого слова.

Если использовать базу знаний, то заранее точно знаем сколько значений у конкретного слова и детали про них.

Pagerank для wordnet o_O.

Часто замеряют на искусственных данных и результаты нерепрезентативны.

Можно это обучать для многословных фраз и для конкретной предметной области.

Тут еще много чего было интересного, но я потерял нить.

<a name="summarization"/>

# Summarization

## Часть 1
wiki статьи, берем статьи их summary. Пытаемся сгенировать похожее.

Берем части подсекций статьи, Биграмы секции, потом документа потом юниграмы. 

Метрики качества предложения и similarity. Много признаков предложения: topic hierarchical model, позиция в тексте и так далее.

LexRank алгоритм.

Замена embeding'ов на кластеры слов.

Rouge 1,2,3,4 оценки.

Memog giannakopoulos 2008

Китайский язык разбиваем ровно по одному символу. Среднее слово в китайском два символа.

Оценка человека для пары summary. Что выглядит лучше.

Multiling15

Weaved baseline (WTF?)

## Часть 2
Используем embeding'и в topic modeling

Content linking: vectors, traditional (???)

LDA

Кластера для предлежений (парное сравнение предложений).

Сумма слов = весу предложения.

## Часть 3
Summarization для отзывов, twitter'а и так далее

Ультра-сжатие (не более 140 символов).

Данные - это контент пользователей. Много жанров.

Удаляем emojis и картинки. Рассматриваем только текст.

Три класса + = -

Ранжирование предложений, sentiment шкала, ранжирование по sentiment'у.

Ручная разметка. Три критерия: содежание, читабельность, responsiveness.

Проблемы: в твиттере много шума, ирония, сарказм.

Корпус 400 документов без cross-checking.

Часть 4: ML подход для оценки summary.

<a name="miltiword"/>

# Multiwords expressions

## Multiword классификация (многоязычная)
Есть compound'ы и есть multiword'ы (нужно различать).

13 классов: организация, локация и так далее.

Рассматриваем классификацию в контексте мониторнига новостей. Выражения рассматриваем вне контекста.

43 языка.

europe media monitor (EMM), name entity hierarchy (события, локации, продукты и так далее).

Брали данные из bablenet, полуавтоматический процесс, маленькая ручная разметка, 200 для родного языка, 200 для инглиша. 5 аннотаторов.

kappa 0.85 (согласованность аннотаторов).

baseline: косинус + простой поиск (?)

Использовали SVM для многих классов

Рассматривали признаки для конкретного языка и языконезависимые признаки.

Рассматривали topic, класс слова, категория(?). Bag of words, tfidf.

Делали бинарную классификацию для каждой пары (зачем?).

10-fold shuffle spli cross falidation

75/25 test/train

SVM + tfidf работают хорошо.

Языко-независимые признаки помогают, для "маленьких" языков языко-зависимые признаки не работают. 

## Двуязычные emebedding'и для извлечения коллокаций
Коллокация - это пара лексических единиц. Один базовый/свободный, а второй зависит от базового.

Для их извлечения нужно обучение. Strong tea/powerful car.

Сложно комбинировать для разных языков, в разных языках используются разные колокации.

Хотим обучить мультиязычные embeddings на параллельных корпусах.

Моноязычные кандидаты, токены->леммы.

word2vec многоязычный.

Используем двуязычные словари, которые содержат данные с вероятностями перевода.

Использовали: лемматизация, POS-tagging, Universal Dependepcies. Рассматриваем паттерны: NOUN+ADJ, VERB+OBJ, NOUN+NOUN.

t-score (?), использовали логарифм правдоподобия.

Выравнила по паттернам (тут потери, так как части речи меняются в зависимости от языка), выбирали N лучших.

Корпус: opensubtitles (OPUS), 13 миллионов предложений.

MaltParser+UD

Linguakit (инструмент для испанского).

Baseline по словарям (?). Видимо, используем переводы конкретных словосочетаний из словаря.

Два ревьюера, для baseline'а согласие 90+, для основного подхода 80.

baseline - хорошая точность, плохое покрытие.

Источники ошибок: preprocess, gender, опечатки, качество модели, антонимы, чай/кофе, mixed varieties (?).

73 F-мера, 88.9 точность.

## Исследование многоязычных лексических ресурсов для задачи про композиционность multiwords
Композицонность. Можем ли получить значение фразы из ее слов. Может не зависить от слов вообще, может от части, можем от всех.

Мультиязычные ресурсы. Token level ресурсы нужны.

Совпадение строк, требует только словари, не требует ничего от языка, не нужны корпуса.

Изпользуем переводы слов, неточное сопадение по морфологии.

Можно мерить схожесть строк разными способами: LCS, lev1, lev2, SW. LCS работает лучше всего.

Итого пытаемся переводить все словосочетание или по слову и смотрим насколько результаты похожи.

Panlex ресурс для переводов, 54 языка.

В работе рассматривали 10 языков, + 10-fold cross-validation.

Результаты хорошие но не идеальные. Результаты сильно зависят от языка.

Используем дистрибутивную семантику aka embeddings. Сравниваем вектор словосочетания со средним вектором по словам.

Комопозиоцнность можно мерить непрерываной величиной.

ENC данные, EVPC данные, GNC данные

Подход через схожесть строк очень помогает в композиции.

Далее были истории про разные значение разных слов и разные способы это выразить/обобщить. Например, слово up.

Итого: комбинация работает лучше всего, из переводной неоднозначности нужно брать лучший вариант, 10 языков потому что 10.

В конце было про PARSEME shared task. Много языков, какая-то лицензия. Хорошо работал transition-based подход.

<a name="ud"/>

# Universal dependencies

## Инфраструктура

universaldependencies.org

70 корпусов,50 языков (есть русские корпуса с эллипсисом).

разные лицензии у разных корпусов

POS + dependecy отношения храним в корпусе 

Много разных типов тектов: новости, wiki, fiction/non fiction, блоги, библия, legal.

Наследовали формат CONLL

Есть metadata уровня предложения.

Явно представлен текст предложения.

В данных присуствуют недревесные зависимости.

Есть дополнительные нулевые вершины (эллиптические).

Все открыто (обсуждения и так далее), все лежит в гите, один корпус-один репозиторий гита.

Есть официальный цикл релизов, много тестов для валидации данных.

Поддерживается целая экосистема для данных: визуализация, аннотации, поиск и так далее.

Syntax net привязан к этим данным.

Медианная точность по именованым связям 81%, в среднем 79%. Японский язык работает хорошо.

Есть web доступ, API доступ и так данее.

PML query tool, например для поиска непроективных конструкций.

udapy для визуализации.

## Использование
Помогает обучать и оценивать парсеры.

Udpipe, syntaxnet - out of box парсеры. Они используют моноязычный подход (?).

Есть многоязычный подход, universal parsing, тренируем на одном языке, тестируем на другом.

Для многоязычного подхода нужны согласованные аннотации. Не всегда нужно хорошее POS-tagging.

тысячи языков без корпусов (?)

wang eisner 2016, the galactic dependencies

Синтез новых языков

Reddy 2016, от синтаксиса к семантике, QA, семантические представления

Vulic 2017, синтаксис для embedding'ов, 

bible корпус содержит 1000 языков

Не требуется много данных для приемлего парсинга.

<a name="topicmodel"/>

# Probability topic models and user behavior

david m. blei

Полезно для организации, визуализации, summarization, поиска, понимания, предсказания (topic models).

Визуализация важна, понимание тематической структуры, аннотация/

Визуализирование коллеции по группе слов.

Topics by time(?), unsupervised подход, сбор событий из мира, аннотирование изображение, анализ генома (???)

Люди читают документы, это можно предсказывать и рекоммендовать.

Документ может соответствовать нескольким темам.

Latent dirichlet allocation (LDA)

Topic - это распределение по словарю.

Для документа можно ввести распределение по topic'ам.

Мы не можем наблюдать всю это.

LDA - это graphical model, поиск факторизации для совместной вероятности.

Для одного слова можно понять его распределение по topic'ам, для документа тоже и для корпуса тоже.

Рассматриваем апостериорную вероятность.

Есть много методов для апроксимации и LDA

Фиксированное число topic'ов.

Если замерять самое частотное слово для конкретного topic'а, то это может быть очень показательным.

Мы хотим получить low dimensional представление документа.

Цель LDA: отнести документ к небольшому количеству topic'ов, и найти небольшое количество слов, которые относятся к topic'у с большой вероятностью. Цели противоречат друг другу.

Есть другие подходы: latent semantic analysis, mixed membership model, PCA. Некоторые независимо придумали генетики o_O

LDA - важная часть многих приложений.

Последние алгоритмы для graphical models расширяются на big data

collaborative topic models, используют LDA

Общие данные, данные о поведении юзеров, статьи которые читал юзер могут помочь людям лучше понять topic'и документа. Помогут находить новые документы, описывать юзеров, находить важные документы.

Подстраиваемся к поведению юзеров, рекоммендуем статьи новым людям несмотря на исходные предположения.

Без текстов нет базовых рекоммдаций, без данных от юзеров мы не можем находить новое.

mendeley.com - ресурс с открытыми текстами.

251k документов, 500 topic'ов, 80k юзеров.

Оценивали работу вручную + спрашивали юзеров.

Arxiv.org данные с кликами. Разные типы кликов.

<a name="coreference"/>

# Coreference

## Interlay of coreference and discourse

Prague книга про coreference.

Много корпусов: MUC, ACE, Ancora, onto notes(?). 

Много типов корпусов

## Три вопроса к анафоре и кореференции
Руслан Митков

Насколько полезно для NLP, как оценивать, какие связи сложно находить

Анафора и кореференция - это разные понятия.

Пытались интергрировать разрешение местоимений в NLP приложения. И оцеивали насколько это помогает.

MARS algorithm

1200 местоимений из 48000 слов.

Для summarization есть небольшоу улучшение.

t тест, статистическая значимость (t test, statistically significant).

Для term extraction получаем +5% f-меры, для классификации +5-10%, но улучшения статистически незначимы.

Гипотетическои можно рассмотреть идеальное разрешение анафоры, это очень помогает для tfidf (статистически значимо +20%, но непонятно для чего именно :) )

Bart algorithm. Если разрешать кореференцию с его помощью то он даже немного ухудшает в одном из приложений.

Иногда улучшает но статистически незначимо.

!Про то как оценивать.

Сложно воспроизводить многие результаты, очень разнятся цифры, согласованность очень низкая

Важно понимать насколько данные сложны и оценивать это.

Смотреть на данные до исследований, делать исследования прозрачными и объективными.

Байка про журнал с негативнми результатами. 

## Кореференция для помощи машинному переводу

TBD

<a name="ir"/>

# Information retrieval and information extraction

TBD
