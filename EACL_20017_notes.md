##### Содержание
[Word embeddings](#word_embeddings)  

<a name="word_embeddings"/>

# Word embeddings (word vector space)

[Слайды](http://people.ds.cam.ac.uk/iv250/tutorial/wv-tutorial.pdf).

## Введение
Иногда word vector space сложно использовать в приложениях - антонимы находятся рядом. Например, мужчина-женщина, запад-восток и так далее.

http://babelnet.org/ - мулиязычный ресурс со словарями, семантикой и базой знаний, synset'ы.

http://paraphrase.org/ - ресурс с большим набором перефразировок.

Unsupervised подходы дешевле - не нужен размеченный корпус.

Нужно объединение двух подходов: unsupervised + размеченные ресурсы.

## Обучение по контексту
Хотим извлекать признаки, которые будут примеными ко многим задачам и инварианты к языку.

Соотвествие слово->вектор в многомерном пространстве.

Слова имеет близкие значения, если они встречаются в одинаковом контексте.

Mikolov статья 2013 года.

Можно использовать негативные примеры в обучении. (случайное слово в контесте из корпуса).

Типы контекстов: bag of words (BOW); с позициями относительно текущего слова; с синтаксическими зависимостями; с синтаксическими зависимостями, но связи с предлогами удаляются.

Cross language dependency based spaces (EACL 2017 статья).

Больше веса для глаголов и пралагательных в контексте, это помогает для улучшения качества. Статья Schwartz NAACL 20016.

Для разных задач помогают разные типы контекстов. Также можно играть с размерностью пространства.

Melamud CONLL 2016, использовал bidirectional LSTM.

Симметричные pattern'ы, статья Schwartz CONLL 2015.

Context2vec.

## Лингвистические ограничения/
TBD.

