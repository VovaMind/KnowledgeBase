# Заметки с EACL 2017

##### Содержание
[Word embeddings](#word_embeddings)  
[Summarization](#summarization)  
[Multiwords expressions](#miltiword)  
[Universal dependencies](#ud)  

<a name="word_embeddings"/>

# Word embeddings (word vector space)

[Слайды](http://people.ds.cam.ac.uk/iv250/tutorial/wv-tutorial.pdf).

## Введение
Иногда word vector space сложно использовать в приложениях - антонимы находятся рядом. Например, мужчина-женщина, запад-восток и так далее.

http://babelnet.org/ - мулиязычный ресурс со словарями, семантикой и базой знаний, synset'ы.

http://paraphrase.org/ - ресурс с большим набором перефразировок.

Unsupervised подходы дешевле - не нужен размеченный корпус.

Нужно объединение двух подходов: unsupervised + размеченные ресурсы.

## Обучение по контексту
Хотим извлекать признаки, которые будут примеными ко многим задачам и инварианты к языку.

Соотвествие слово->вектор в многомерном пространстве.

Слова имеет близкие значения, если они встречаются в одинаковом контексте.

Mikolov статья 2013 года.

Можно использовать негативные примеры в обучении. (случайное слово в контесте из корпуса).

Типы контекстов: bag of words (BOW); с позициями относительно текущего слова; с синтаксическими зависимостями; с синтаксическими зависимостями, но связи с предлогами удаляются.

Cross language dependency based spaces (EACL 2017 статья).

Больше веса для глаголов и пралагательных в контексте, это помогает для улучшения качества. Статья Schwartz NAACL 20016.

Для разных задач помогают разные типы контекстов. Также можно играть с размерностью пространства.

Melamud CONLL 2016, использовал bidirectional LTSM.

Симметричные pattern'ы, статья Schwartz CONLL 2015.

Context2vec.

## Лингвистические ограничения
Все зависит от корпуса.

Используем структурированые знания (источники?).

Wordnet - это набор synset'ов, каждый из которых соответствует некоторому концепту. Слово может участвовать в нескольких synset'ах. Внутри wordnet'а есть отношения: гипероним, гипоним, принадлежность к domain'у, мероним (часть-целое) и так далее. Гиперонимы образуют иерархию. Прилагательные сложно ложаться на иерархию.

Проблемы источников типа wordnet: дорого обновлять, ограниченный словарь, плохо описывают конкретные предметные области и последние изменения в языке.

Wikipedia полезный источник данных. Есть ссылки, многоязычность много сырой и структурированной информации.

Wictionary намного больше, чем wordnet, хорошое покрытие языков, очень избыточные.

omegawiki близко к wordnet'у, плохое покрытие.

Bablenet - это слияние всех источников, похоже на wordnet, есть API. Есть live-версия (последняя, видимо, не очень стабильная).

Paraphrase база с шаблонами на 16 языках, извлекалась из параллельных корпусов. Несколько типов: лексический, перефразировка, синтаксический с нетерминалами. В базе есть scores.

Google ngram база. Ранжированные перефразировки.

Wordnet имеет хорошее качество.

## Комбинирование подходов
Обобщать по конкретному словарю сложно.

Можно соединить два подхода. Основные способы: "тяжелый" (все делаем совместно), пост-обработка (после построения векторов двигаем их).

Оба подхода используют одинаковые входные данные.

Yu and Dredze ACL 2014.

Интерполяции по источникам знаний.

Парное сходство по примерам. Fool/stupid ближе, чем fool/smart.

Нужно отличать similarity, relatedness и association. Для классификации и topic modeling можно не отличать.

Пост-обработка. Двигаем слова после построения векторов и оптимизируем фунцию для пар синонимов. При этом не должны сильно поменять построенные вектора.

Mrksic NAACL 2016. Функция оценки, которая базируется на синонимах, антонимах и исходных векторах.

Wordnet хорошо для одноязычного случая, babelnet для многоязычного.

Paragram подход (Wieting 2016) добавляем "негативные" пары синонимов (вроде бы, случайные).

Ограничения для конкретной модели помогают с омонимией.

## Последняя часть
Если использовать только ограничения и случайные вектора, то это может неплохо работать (очень зависят от языка).

Многоязычные вектора работают неплохо даже при условии недостатка ресурсов.

SimLex данные (есть для русского). Не для всех языков хватает данных.

Диалог + отслеживание его состояния (slots, тип вопроса). Рассматриваем диалог как распределение таких состояний. Делать по точным совпадаением помогает очень плохо. Семантичесие словари помогают для маленьких данных. Можно обучить нейросеть.

offtop: можно перейти от классификации на много классов к большому количеству бинарных классификаций. Relu фунция активации max(0,x). CNN помогает обучать представления. Сегменты слов в NN. 

Wizard of Oz dialog dataset.

Glove vectors.

Морфология с небольшой коллекцией размеченных данные работает хорошо. Оченб помогает для немецкого.

Vendrov ICLR 2016 (частичный порядок в векторном пространстве).

## Омонимия
Есть проблема с омонимией.

Разделять значения для слов. 

Можно делать unsupervised кластеризуя слова по контекстам. Нужно зарнее выбрать число кластеров (число значений) для каждого слова.

Если использовать базу знаний, то заранее точно знаем сколько значений у конкретного слова и детали про них.

Pagerank для wordnet o_O.

Часто замеряют на искусственных данных и результаты нерепрезентативны.

Можно это обучать для многословных фраз и для конкретной предметной области.

Тут еще много чего было интересного, но я потерял нить.

<a name="summarization"/>

# Summarization

## Часть 1
wiki статьи, берем статьи их summary. Пытаемся сгенировать похожее.

Берем части подсекций статьи, Биграмы секции, потом документа потом юниграмы. 

Метрики качества предложения и similarity. Много признаков предложения: topic hierarchical model, позиция в тексте и так далее.

LexRank алгоритм.

Замена embeding'ов на кластеры слов.

Rouge 1,2,3,4 оценки.

Memog giannakopoulos 2008

Китайский язык разбиваем ровно по одному символу. Среднее слово в китайском два символа.

Оценка человека для пары summary. Что выглядит лучше.

Multiling15

Weaved baseline (WTF?)

## Часть 2
Используем embeding'и в topic modeling

Content linking: vectors, traditional (???)

LDA

Кластера для предлежений (парное сравнение предложений).

Сумма слов = весу предложения.

## Часть 3
Summarization для отзывов, twitter'а и так далее

Ультра-сжатие (не более 140 символов).

Данные - это контент пользователей. Много жанров.

Удаляем emojis и картинки. Рассматриваем только текст.

Три класса + = -

Ранжирование предложений, sentiment шкала, ранжирование по sentiment'у.

Ручная разметка. Три критерия: содежание, читабельность, responsiveness.

Проблемы: в твиттере много шума, ирония, сарказм.

Корпус 400 документов без cross-checking.

Часть 4: ML подход для оценки summary.

<a name="miltiword"/>

# Multiwords expressions

## Multiword классификация (многоязычная)
Есть compound'ы и есть multiword'ы (нужно различать).

13 классов: организация, локация и так далее.

Рассматриваем классификацию в контексте мониторнига новостей. Выражения рассматриваем вне контекста.

43 языка.

europe media monitor (EMM), name entity hierarchy (события, локации, продукты и так далее).

Брали данные из bablenet, полуавтоматический процесс, маленькая ручная разметка, 200 для родного языка, 200 для инглиша. 5 аннотаторов.

kappa 0.85 (согласованность аннотаторов).

baseline: косинус + простой поиск (?)

Использовали SVM для многих классов

Рассматривали признаки для конкретного языка и языконезависимые признаки.

Рассматривали topic, класс слова, категория(?). Bag of words, tfidf.

Делали бинарную классификацию для каждой пары (зачем?).

10-fold shuffle spli cross falidation

75/25 test/train

SVM + tfidf работают хорошо.

Языко-независимые признаки помогают, для "маленьких" языков языко-зависимые признаки не работают. 

## Двуязычные emebedding'и для извлечения коллокаций
Коллокация - это пара лексических единиц. Один базовый/свободный, а второй зависит от базового.

Для их извлечения нужно обучение. Strong tea/powerful car.

Сложно комбинировать для разных языков, в разных языках используются разные колокации.

Хотим обучить мультиязычные embeddings на параллельных корпусах.

Моноязычные кандидаты, токены->леммы.

word2vec многоязычный.

Используем двуязычные словари, которые содержат данные с вероятностями перевода.

Использовали: лемматизация, POS-tagging, Universal Dependepcies. Рассматриваем паттерны: NOUN+ADJ, VERB+OBJ, NOUN+NOUN.

t-score (?), использовали логарифм правдоподобия.

Выравнила по паттернам (тут потери, так как части речи меняются в зависимости от языка), выбирали N лучших.

Корпус: opensubtitles (OPUS), 13 миллионов предложений.

MaltParser+UD

Linguakit (инструмент для испанского).

Baseline по словарям (?). Видимо, используем переводы конкретных словосочетаний из словаря.

Два ревьюера, для baseline'а согласие 90+, для основного подхода 80.

baseline - хорошая точность, плохое покрытие.

Источники ошибок: preprocess, gender, опечатки, качество модели, антонимы, чай/кофе, mixed varieties (?).

73 F-мера, 88.9 точность.

## Исследование многоязычных лексических ресурсов для задачи про композиционность multiwords
Композицонность. Можем ли получить значение фразы из ее слов. Может не зависить от слов вообще, может от части, можем от всех.

Мультиязычные ресурсы. Token level ресурсы нужны.

Совпадение строк, требует только словари, не требует ничего от языка, не нужны корпуса.

Изпользуем переводы слов, неточное сопадение по морфологии.

Можно мерить схожесть строк разными способами: LCS, lev1, lev2, SW. LCS работает лучше всего.

Итого пытаемся переводить все словосочетание или по слову и смотрим насколько результаты похожи.

Panlex ресурс для переводов, 54 языка.

В работе рассматривали 10 языков, + 10-fold cross-validation.

Результаты хорошие но не идеальные. Результаты сильно зависят от языка.

Используем дистрибутивную семантику aka embeddings. Сравниваем вектор словосочетания со средним вектором по словам.

Комопозиоцнность можно мерить непрерываной величиной.

ENC данные, EVPC данные, GNC данные

Подход через схожесть строк очень помогает в композиции.

Далее были истории про разные значение разных слов и разные способы это выразить/обобщить. Например, слово up.

Итого: комбинация работает лучше всего, из переводной неоднозначности нужно брать лучший вариант, 10 языков потому что 10.

В конце было про PARSEME shared task. Много языков, какая-то лицензия. Хорошо работал transition-based подход.

<a name="ud"/>

# Universal dependencies

TBD

