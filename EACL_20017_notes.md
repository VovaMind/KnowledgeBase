##### Содержание
[Word embeddings](#word_embeddings)  

<a name="word_embeddings"/>

# Word embeddings (word vector space)

[Слайды](http://people.ds.cam.ac.uk/iv250/tutorial/wv-tutorial.pdf).

## Введение
Иногда word vector space сложно использовать в приложениях - антонимы находятся рядом. Например, мужчина-женщина, запад-восток и так далее.

http://babelnet.org/ - мулиязычный ресурс со словарями, семантикой и базой знаний, synset'ы.

http://paraphrase.org/ - ресурс с большим набором перефразировок.

Unsupervised подходы дешевле - не нужен размеченный корпус.

Нужно объединение двух подходов: unsupervised + размеченные ресурсы.

## Обучение по контексту
Хотим извлекать признаки, которые будут примеными ко многим задачам и инварианты к языку.

Соотвествие слово->вектор в многомерном пространстве.

Слова имеет близкие значения, если они встречаются в одинаковом контексте.

Mikolov статья 2013 года.

Можно использовать негативные примеры в обучении. (случайное слово в контесте из корпуса).

Типы контекстов: bag of words (BOW); с позициями относительно текущего слова; с синтаксическими зависимостями; с синтаксическими зависимостями, но связи с предлогами удаляются.

Cross language dependency based spaces (EACL 2017 статья).

Больше веса для глаголов и пралагательных в контексте, это помогает для улучшения качества. Статья Schwartz NAACL 20016.

Для разных задач помогают разные типы контекстов. Также можно играть с размерностью пространства.

Melamud CONLL 2016, использовал bidirectional LTSM.

Симметричные pattern'ы, статья Schwartz CONLL 2015.

Context2vec.

## Лингвистические ограничения
Все зависит от корпуса.

Используем структурированые знания (источники?).

Wordnet - это набор synset'ов, каждый из которых соответствует некоторому концепту. Слово может участвовать в нескольких synset'ах. Внутри wordnet'а есть отношения: гипероним, гипоним, принадлежность к domain'у, мероним (часть-целое) и так далее. Гиперонимы образуют иерархию. Прилагательные сложно ложаться на иерархию.

Проблемы источников типа wordnet: дорого обновлять, ограниченный словарь, плохо описывают конкретные предметные области и последние изменения в языке.

Wikipedia полезный источник данных. Есть ссылки, многоязычность много сырой и структурированной информации.

Wictionary намного больше, чем wordnet, хорошое покрытие языков, очень избыточные.

omegawiki близко к wordnet'у, плохое покрытие.

TBD

